{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "[Jigsaw Multilingual Toxic Comment Classification](https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification) was the 3rd annual competition organized by the Jigsaw team. It followed [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge), the original 2018 competition, and [Jigsaw Unintended Bias in Toxicity Classification](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification). <br> \n",
    "This year, the goal was to run multilingual toxicity predictions on six different languages (i.e. es, tr, it, ru, pt and fr), and promote the usage of TPU.\n",
    "\n",
    "My team-mate [@leecming](https://www.kaggle.com/leecming) and I won the first place in the competition, and I am very proud of the achievement.\n",
    "This notebook represents a template for the initial baseline that I used, for the languages that had a separate validation data ***(es, it, tr)***. <br>\n",
    "I was highly inspired by a few kernels (see references below), from which a big part of my baseline code is derived, for which I am thankful.\n",
    "\n",
    "Notebook [Jigsaw TPU: XLM-Roberta](https://www.kaggle.com/xhlulu/jigsaw-tpu-xlm-roberta) showcased a multilingual model as one of the first in the competition, and served as guideline for many of the competitors including myself. \n",
    "The main changes in my template baseline compared to reference:\n",
    "1. The translated dataset [Jigsaw Train Multilingual Coments (Google API)](https://www.kaggle.com/miklgr500/jigsaw-train-multilingual-coments-google-api) is used for training, instead of original English-only. In this example, I show how to train on the Spanish translated dataset <br>\n",
    "2. Making use of a custom training loop on TPU\n",
    "3. Parallel processing for faster tokenization\n",
    "4. Balancing the non-toxic vs. toxic comments as 4:1 (i.e 0.8 fraction of non-toxic and 0.2 fraction of toxic)\n",
    "5. Divide one epoch in several iterations\n",
    "6. Make test predictions using the model with best AUC on validation\n",
    "7. Printing out more metrics during training (TP/TN/FP/FN & AUC)\n",
    "8. Stage 2: further resume train on validation data with best model (higher score on LB) \n",
    "9. Further upgrade to initial baseline: predict with repeated psuedolabels\n",
    "\n",
    "\\* Note: This notebook is used for the case of the languages where a separate validation dataset is given (es, tr, it), where the validation dataset is used for validating the model. For the remaining languages (ru, pt, fr) a simple average was taken over several snapshots ensembles. This is shown in another notebook.\n",
    "\n",
    "\n",
    "### References: notebooks and guides\n",
    "1. [Jigsaw TPU: XLM-Roberta](https://www.kaggle.com/xhlulu/jigsaw-tpu-xlm-roberta)\n",
    "2. [Custom Training Loop with 100+ flowers on TPU](https://www.kaggle.com/mgornergoogle/custom-training-loop-with-100-flowers-on-tpu)\n",
    "3. [Tensorflow guide (useful for balancing datasets)](https://www.tensorflow.org/guide/data)\n",
    "4. [Hugginface models](https://huggingface.co/models)\n",
    "\n",
    "### References: datasets\n",
    "1. [Jigsaw Train Multilingual Coments (Google API)](https://www.kaggle.com/miklgr500/jigsaw-train-multilingual-coments-google-api)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# General imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Transformers\n",
    "from transformers import TFAutoModel, TFBertModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TPU and configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect hardware, return appropriate distribution strategy\n",
    "try:\n",
    "    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
    "    # set: this is always the case on Kaggle.\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Configuration parameters\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n",
    "MAX_LEN = 192\n",
    "MODEL = 'jplu/tf-xlm-roberta-large' # for BERT model replace by e.g. dccuchile/bert-base-spanish-wwm-uncased\n",
    "LANG = \"es\" # can be any of es, it, tr in this notebook\n",
    "CONSTANT_LR = 3e-6 # 3e-6 generally good. Set lower e.g. 1e-6 for more finetuning\n",
    "BALANCEMENT = [0.8, 0.2] # non-toxic vs. toxic\n",
    "BERT_MODEL = False # specify if the given model is a BERT model\n",
    "N_EPOCHS = 3 # 3-5 epochs are usually enough. Set higher e.g. 5 for more finetuning\n",
    "N_ITER_PER_EPOCH = 10\n",
    "PREDICT_START_ITER = 10 # start iteration to predict on test. best iterations around +-20 (2 full epochs)\n",
    "STAGE2 = True # resume training with checkpoint of best model\n",
    "\n",
    "REPEAT_PL = 0 # Upgrade: repeat PL with train (I repeated 6x on my last subs). Default=0 (no pseudolabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def regular_encode(texts, max_len):\n",
    "    \"\"\"\n",
    "    Tokenizing the texts into their respective IDs using regular batch encoding\n",
    "    \n",
    "    Accepts: * texts: the text to be tokenize\n",
    "             * max_len: max length of text\n",
    "    \n",
    "    Returns: * array of tokenized IDs \n",
    "    \"\"\"\n",
    "    enc_di = tokenizer.batch_encode_plus(\n",
    "        texts, \n",
    "        return_attention_masks=False, \n",
    "        return_token_type_ids=False,\n",
    "        pad_to_max_length=True,\n",
    "        max_length=max_len\n",
    "    )\n",
    "    \n",
    "    return np.array(enc_di['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parallel_encode(texts, max_len):\n",
    "    \"\"\"\n",
    "    Tokenizing the texts into their respective IDs using parallel processing\n",
    "    \n",
    "    Accepts: * texts: the text to be tokenized\n",
    "             * max_len: max length of text\n",
    "    \n",
    "    Returns: * array of tokenized IDs + the toxicity label  \n",
    "    \"\"\"\n",
    "    enc_di = tokenizer.encode_plus(\n",
    "        str(texts[0]),\n",
    "        return_attention_masks=False, \n",
    "        return_token_type_ids=False,\n",
    "        pad_to_max_length=True,\n",
    "        max_length=max_len\n",
    "    )\n",
    "    return np.array(enc_di['input_ids']), texts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(transformer, max_len):\n",
    "    \"\"\"\n",
    "    Build the model by using transformer layer and simple CLS token\n",
    "    \n",
    "    Accepts: * transformer: transformer layer\n",
    "             * max_len: max length of text\n",
    "    \n",
    "    Returns: * model \n",
    "    \"\"\"\n",
    "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    sequence_output = transformer(input_word_ids)[0]\n",
    "    cls_token = sequence_output[:, 0, :]\n",
    "    out = Dense(1, activation='sigmoid')(cls_token)\n",
    "    model = Model(inputs=input_word_ids, outputs=out)    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create fast tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First load the real tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load text data into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(f\"/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-{LANG}-cleaned.csv\")\n",
    "valid = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation.csv')\n",
    "test = pd.read_csv(\"/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if REPEAT_PL:\n",
    "    sub = pd.read_csv(\"../input/external-sub-xlm-r-9383/sub-Jigsaw-tpu-xlm-roberta-lb9383.csv\") # use one of earlier subs e.g. sub taken from https://www.kaggle.com/xhlulu/jigsaw-tpu-xlm-roberta\n",
    "    sub[\"comment_text\"] = test[\"content\"]\n",
    "    sub = sub.loc[test[\"lang\"]==LANG].reset_index(drop=True)\n",
    "    sub_repeat = pd.concat([sub]*REPEAT_PL, ignore_index=True) # repeat PL multipe times for training\n",
    "    same_cols = [\"comment_text\", \"toxic\"]\n",
    "    train = pd.concat([train[same_cols], sub_repeat[same_cols]]).sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get specific validation and test\n",
    "valid = valid.loc[valid[\"lang\"]==LANG].reset_index(drop=True)\n",
    "test = test.loc[test[\"lang\"]==LANG].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "# Tokenize train with parallel processing\n",
    "rows = zip(train['comment_text'].values.tolist(), train.toxic.values.tolist())\n",
    "x_y_train = Parallel(n_jobs=4, backend='multiprocessing')(delayed(parallel_encode)(row, max_len=MAX_LEN) for row in tqdm(rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.vstack(np.array(x_y_train)[:,0])\n",
    "\n",
    "y_train = np.array(x_y_train)[:,1]\n",
    "y_train = np.asarray(y_train).astype('float32').reshape((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Tokenize valid regular processing\n",
    "x_valid = regular_encode(valid.comment_text.values, max_len=MAX_LEN)\n",
    "\n",
    "y_valid = valid.toxic.values\n",
    "y_valid = np.asarray(y_valid).astype('float32').reshape((-1,1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "x_test = regular_encode(test.content.values, max_len=MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build datasets objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train and valid dataset\n",
    "train_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((x_train, y_train))\n",
    "    .shuffle(buffer_size=len(x_train), seed = 18)\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "valid_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((x_valid, y_valid))\n",
    "    .batch(BATCH_SIZE)\n",
    "    .cache()\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "test_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices(x_test)\n",
    "    .batch(BATCH_SIZE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Balance the train dataset by creating seperate negative and positive datasets. \n",
    "# Note: tf.squeeze remove the added dim to labels\n",
    "# Example taken from https://www.tensorflow.org/guide/data\n",
    "\n",
    "negative_ds = (\n",
    "  train_dataset\n",
    "    .filter(lambda _, y: tf.squeeze(y)==0)\n",
    "    .repeat())\n",
    "\n",
    "positive_ds = (\n",
    "  train_dataset\n",
    "    .filter(lambda _, y: tf.squeeze(y)==1)\n",
    "    .repeat())\n",
    "\n",
    "balanced_ds = tf.data.experimental.sample_from_datasets(\n",
    "    [negative_ds, positive_ds], BALANCEMENT).batch(BATCH_SIZE) # Around 80%/20% to be expected for 0/1 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# distribute the datset according to the strategy\n",
    "train_dist_ds = strategy.experimental_distribute_dataset(balanced_ds)\n",
    "valid_dist_ds = strategy.experimental_distribute_dataset(valid_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions TF custom training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instantiate metrics\n",
    "with strategy.scope():\n",
    "    # Accuracy, AUC, loss train\n",
    "    train_accuracy = tf.keras.metrics.BinaryAccuracy()\n",
    "    train_auc = tf.keras.metrics.AUC()\n",
    "    train_loss = tf.keras.metrics.Sum()\n",
    "    \n",
    "    # Accuracy, AUC, loss valid\n",
    "    valid_accuracy = tf.keras.metrics.BinaryAccuracy()\n",
    "    valid_auc = tf.keras.metrics.AUC()\n",
    "    valid_loss = tf.keras.metrics.Sum()\n",
    "    \n",
    "    # TP, TN, FN, FP train\n",
    "    train_TP = tf.keras.metrics.TruePositives()\n",
    "    train_TN = tf.keras.metrics.TrueNegatives()\n",
    "    train_FP = tf.keras.metrics.FalsePositives()\n",
    "    train_FN = tf.keras.metrics.FalseNegatives()\n",
    "    \n",
    "    # TP, TN, FN, FP valid\n",
    "    valid_TP = tf.keras.metrics.TruePositives()\n",
    "    valid_TN = tf.keras.metrics.TrueNegatives()\n",
    "    valid_FP = tf.keras.metrics.FalsePositives()\n",
    "    valid_FN = tf.keras.metrics.FalseNegatives()\n",
    "    \n",
    "    # Loss function and optimizer\n",
    "    loss_fn = lambda a,b: tf.nn.compute_average_loss(tf.keras.losses.binary_crossentropy(a,b), global_batch_size=BATCH_SIZE)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=CONSTANT_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(tokens, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        probabilities = model(tokens, training=True)\n",
    "        loss = loss_fn(labels, probabilities)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    \n",
    "    # update metrics\n",
    "    train_accuracy.update_state(labels, probabilities)\n",
    "    train_auc.update_state(labels, probabilities)\n",
    "    train_loss.update_state(loss)\n",
    "    \n",
    "    train_TP.update_state(labels, probabilities)\n",
    "    train_TN.update_state(labels, probabilities)\n",
    "    train_FP.update_state(labels, probabilities)\n",
    "    train_FN.update_state(labels, probabilities)\n",
    "    \n",
    "@tf.function\n",
    "def valid_step(tokens, labels):\n",
    "    probabilities = model(tokens, training=False)\n",
    "    loss = loss_fn(labels, probabilities)\n",
    "    \n",
    "    # update metrics\n",
    "    valid_accuracy.update_state(labels, probabilities)\n",
    "    valid_auc.update_state(labels, probabilities)\n",
    "    valid_loss.update_state(loss)\n",
    "    \n",
    "    valid_TP.update_state(labels, probabilities)\n",
    "    valid_TN.update_state(labels, probabilities)\n",
    "    valid_FP.update_state(labels, probabilities)\n",
    "    valid_FN.update_state(labels, probabilities)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "with strategy.scope():\n",
    "    if BERT_MODEL:\n",
    "        transformer_layer = TFBertModel.from_pretrained(MODEL, from_pt=True)\n",
    "    else:\n",
    "        transformer_layer = TFAutoModel.from_pretrained(MODEL)\n",
    "    model = build_model(transformer_layer, max_len=MAX_LEN)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VALIDATION_STEPS = x_valid.shape[0] // BATCH_SIZE\n",
    "STEPS_PER_EPOCH = x_train.shape[0] // (BATCH_SIZE*N_ITER_PER_EPOCH)\n",
    "print(\"Steps per epoch:\", STEPS_PER_EPOCH)\n",
    "EPOCHS = N_EPOCHS*N_ITER_PER_EPOCH\n",
    "\n",
    "best_auc = 0\n",
    "epoch = 0\n",
    "\n",
    "preds_all = []\n",
    "for step, (tokens, labels) in enumerate(train_dist_ds):\n",
    "    # run training step\n",
    "    strategy.experimental_run_v2(train_step, args=(tokens, labels))\n",
    "    print('=', end='', flush=True)\n",
    "    \n",
    "    # print metrics training\n",
    "    if ((step+1) // STEPS_PER_EPOCH) > epoch:\n",
    "        print(\"\\n Epoch:\", epoch)\n",
    "        print('|', end='', flush=True)\n",
    "        print(\"TP -  TN  -  FP  -  FN\")\n",
    "        print(train_TP.result().numpy(), train_TN.result().numpy(), train_FP.result().numpy(), train_FN.result().numpy())\n",
    "        print(\"train AUC: \",train_auc.result().numpy())\n",
    "        print(\"train loss: \", train_loss.result().numpy() / STEPS_PER_EPOCH)\n",
    "        \n",
    "        # validation run for es, it, tr and save model\n",
    "        for tokens, labels in valid_dist_ds:\n",
    "            strategy.experimental_run_v2(valid_step, args=(tokens, labels))\n",
    "            print('=', end='', flush=True)\n",
    "\n",
    "        # compute metrics\n",
    "        print(\"\\n\")\n",
    "        print(\"TP -  TN  -  FP  -  FN\")\n",
    "        print(valid_TP.result().numpy(), valid_TN.result().numpy(), valid_FP.result().numpy(), valid_FN.result().numpy())\n",
    "        print(\"val AUC: \", valid_auc.result().numpy())\n",
    "        print(\"val loss: \", valid_loss.result().numpy() / VALIDATION_STEPS)\n",
    "\n",
    "        # Save predictions and weights of model\n",
    "        if (valid_auc.result().numpy() > best_auc) & (epoch>=PREDICT_START_ITER):\n",
    "            best_auc = valid_auc.result().numpy()\n",
    "            print(\"Prediction on test set - snapshot\")\n",
    "            preds = model.predict(test_dataset, verbose = 1)\n",
    "            preds_all.append(preds)\n",
    "            model.save_weights('best_model.h5') # keep track of best model\n",
    "        # set up next epoch\n",
    "        epoch = (step+1) // STEPS_PER_EPOCH\n",
    "\n",
    "        train_auc.reset_states()\n",
    "        valid_auc.reset_states()\n",
    "\n",
    "        valid_loss.reset_states()\n",
    "        train_loss.reset_states()\n",
    "        \n",
    "        train_TP.reset_states()\n",
    "        train_TN.reset_states()\n",
    "        train_FP.reset_states()\n",
    "        train_FN.reset_states()\n",
    "        \n",
    "        valid_TP.reset_states()\n",
    "        valid_TN.reset_states()\n",
    "        valid_FP.reset_states()\n",
    "        valid_FN.reset_states()\n",
    "        \n",
    "        if epoch >= EPOCHS:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generate averages of predictions: last one, and average of snapshots\n",
    "test[\"toxic_best\"] = preds_all[-1]\n",
    "test[\"toxic_mean_snap\"] = sum(preds_all)/len(preds_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the predictions\n",
    "MODEL_NAME = MODEL.replace(\"/\", \"-\")\n",
    "test.to_csv(f\"test-{LANG}-{MODEL_NAME}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2: resume training on validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build datasets objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if STAGE2:\n",
    "    # the validation set becomes train_dataset\n",
    "    train_dataset = (\n",
    "        tf.data.Dataset\n",
    "        .from_tensor_slices((x_valid, y_valid)) # replaced by x_valid, y_valid!\n",
    "        .shuffle(buffer_size=len(x_valid), seed = 18)\n",
    "        .prefetch(AUTO)\n",
    "        .batch(BATCH_SIZE)\n",
    "        .repeat()\n",
    "    )\n",
    "    \n",
    "    # distribute the datset according to the strategy\n",
    "    train_dist_ds = strategy.experimental_distribute_dataset(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if STAGE2:\n",
    "    model.load_weights(\"best_model.h5\") # best model from stage1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if STAGE2:\n",
    "    STEPS_PER_EPOCH = round(x_valid.shape[0] / (BATCH_SIZE*N_ITER_PER_EPOCH))\n",
    "    print(\"Steps per epoch:\", STEPS_PER_EPOCH)\n",
    "    EPOCHS = N_EPOCHS*N_ITER_PER_EPOCH\n",
    "    best_auc = 0\n",
    "    epoch = 0\n",
    "\n",
    "    preds_all = []\n",
    "    for step, (tokens, labels) in enumerate(train_dist_ds):\n",
    "        # run training step\n",
    "        strategy.experimental_run_v2(train_step, args=(tokens, labels))\n",
    "        print('=', end='', flush=True)\n",
    "\n",
    "        # print metrics training\n",
    "        if ((step+1) // STEPS_PER_EPOCH) > epoch:\n",
    "            print(\"\\n Epoch:\", epoch)\n",
    "            print('|', end='', flush=True)\n",
    "            print(\"TP -  TN  -  FP  -  FN\")\n",
    "            print(train_TP.result().numpy(), train_TN.result().numpy(), train_FP.result().numpy(), train_FN.result().numpy())\n",
    "            print(\"train AUC: \",train_auc.result().numpy())\n",
    "            print(\"train loss: \", train_loss.result().numpy() / STEPS_PER_EPOCH)\n",
    "\n",
    "            # Save predictions and weights of model\n",
    "            if epoch>=PREDICT_START_ITER:\n",
    "                print(\"Prediction on test set - snapshot\")\n",
    "                preds = model.predict(test_dataset, verbose = 1)\n",
    "                preds_all.append(preds)\n",
    "                \n",
    "            # set up next epoch\n",
    "            epoch = (step+1) // STEPS_PER_EPOCH\n",
    "            \n",
    "            train_auc.reset_states()\n",
    "            train_loss.reset_states()\n",
    "\n",
    "            train_TP.reset_states()\n",
    "            train_TN.reset_states()\n",
    "            train_FP.reset_states()\n",
    "            train_FN.reset_states()\n",
    "            \n",
    "            if epoch >= EPOCHS:\n",
    "                # save model if needed\n",
    "                model.save_weights('best_model_valid.h5') \n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if STAGE2:\n",
    "    #Generate averages of snapshot\n",
    "    test[\"toxic_mean_snap_valid\"] = sum(preds_all)/len(preds_all)\n",
    "    # Save the predictions\n",
    "    test.to_csv(f\"test-{LANG}-{MODEL_NAME}.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
